{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Model-Server A Pure python-3 based flexible gRPC server for hosting Deep Learning, Machine Learning models trained on any framework! The documentation can be found here https://abhijit-2592.github.io/model-server/ Installation Method 1: Installing from python pip pip3 install model-server Method 2: Creating wheel from github clone the repository Run bash create_pip_wheel_and_upload.sh . This will prompt for userid and password. You can ctrl-c this Then install the created wheel Method 3: No installation. Using the source code directly. If this is the case, you need to compile the protobufs. Run bash compile_protobufs.sh . Then add the project root to your $PYTHONPATH . Note: Method 2 and 3 requires libprotoc>=3.6.0 Why Model-Server? Taking deep learning models to production at scale is not a very straight forward process. If you are using Tensorflow then you have Tensorflow Serving . But, if you are using other frameworks like PyTorch , MXNet , scikit-learn etc. Taking your model to production is not very straight forward (Flask, Django and other ReST frameworks). Ideally you should be able to extend Tensorflow Serving to support models from other frameworks also but, this is extremly cumbersome! Thus, to bridge this gap we have Model-Server : A high performance framework neutral serving solution! The idea is: if you are able to train your model in python you should be able to deploy at scale using pure python Salient Features Model-Server is heavily inspired from Tensorflow Serving Out of box client side batching support Pure python implementation : You don't need to fiddle around with C++ to have a scalable deployment solution Framework neutral : Using PyTorch , MXNet etc? Don't worry! The solution is platform neutral. If you can use a framework to train in python-3 , Model-Server can deploy it at scale Single server for multi-framework and multi-models : You can host multiple models using the same framework or a mixture of multiple PyTorch , MXNet , scikit-learn Tensorflow etc models! Getting started The core of Model Server is a Servable . A servable is nothing but a python class containing your model's prediction definition which will be served by the Model-Server . All servables must inherit from model_server.Servable for the Model-Server to serve it. To deploy your model to production with Model Server , you just have to write a single python-3 file containing a class which inherits from model_server.Servable and has the following two methods: predict(self, input_array_dict) get_model_info(self, list_of_model_info_dict) Now run the floowing to start the server in 5001 port python -m model_server.runserver path_to_custom_servable_file.py For more info on command line arguments: python -m model_server.runserver --help A simple example create a file called simple_servable.py with the following contents: import numpy as np from model_server import Servable class my_custom_servable(Servable): def __init__(self, args): # args contains values from ArgumentParser # Thus you can pass any kwargs via command line and you get them here pass def predict(self, input_array_dict): \"\"\"This method is responsible for the gRPC call GetPredictions(). All custom servables must define this method. Arguments: input_array_dict (dict): The PredictionRequest proto decoded as a python dictionary. # example input_array_dict = { \"input_tensor_name1\": numpy array, \"input_tensor_name2\": numpy array } Returns: A python dictionary with key (typically output name) and value as numpy array of predictions # example output = { \"output_tensor_name1\": numpy array, \"output_tensor_name2\": numpy array } \"\"\" print(input_array_dict) return ({\"output_array1\": np.array([100, 200]).astype(np.float32), \"output_array2\": np.array([\"foo\".encode(),\"bar\".encode()]).astype(object), # you can get and pass strings encoded as bytes also }) def get_model_info(self, list_of_model_info_dict): \"\"\"This method which is responsible for the call GetModelInfo() Arguments: list_of_model_info_dict (list/tuple): A list containing model_info_dicts Note: model_info_dict contains the following keys: { \"name\": \"model name as string\" \"version\": \"version as string\" \"status\": \"status string\" \"misc\": \"string with miscellaneous info\" } Returns: list_of_model_info_dict (dict): containing the model and server info. This is similar to the function input \"\"\" return [{\"name\": \"first_model\", \"version\": 1, \"status\": \"up\"}, {\"name\": \"second_model\", \"version\": 2, \"status\": \"up\", \"misc\": \"Other miscellaneous details\"}] Now run: python -m model_server.runserver path/to/simple_servable.py To start the gRPC server! Now let's define the client! import grpc import numpy as np from model_server import server_pb2, server_pb2_grpc from model_server.utils import create_tensor_proto from model_server.utils import create_predict_request from model_server.utils import create_array_from_proto from model_server.utils import create_model_info_proto channel = grpc.insecure_channel('localhost:5001') # default port # create a stub (client) stub = server_pb2_grpc.ModelServerStub(channel) input_array_dict = {\"input1\":create_tensor_proto(np.array([1,2]).astype(np.uint8)), \"input2\":create_tensor_proto(np.array([[10.0,11.0], [12.0,13.0]]).astype(np.float32)), \"input3\":create_tensor_proto(np.array([\"Hi\".encode(), \"Hello\".encode(), \"test\".encode()]).astype(object)) } # create the prediction request predict_request= create_predict_request(input_array_dict, name=\"simple_call\") # make the call response = stub.GetPredictions(predict_request) # decode the response print(create_array_from_proto(response.outputs[\"output_array1\"])) # prints: array([100., 200.], dtype=float32) # Getting the model status model_info_proto = create_model_info_proto([]) # you can pass an empty list also response = stub.GetModelInfo(model_info_proto) Look at examples folder for further examples Work in Progress Support server side batching and async calls. Provide a gRPC endpoint for Active Learning so that you can plug in Model Server with your labeling tool and train on fly! Provide a ReST wrapper Feel free to file issues, provide suggestions and pull requests","title":"Home"},{"location":"#model-server","text":"A Pure python-3 based flexible gRPC server for hosting Deep Learning, Machine Learning models trained on any framework! The documentation can be found here https://abhijit-2592.github.io/model-server/","title":"Model-Server"},{"location":"#installation","text":"","title":"Installation"},{"location":"#method-1","text":"Installing from python pip pip3 install model-server","title":"Method 1:"},{"location":"#method-2","text":"Creating wheel from github clone the repository Run bash create_pip_wheel_and_upload.sh . This will prompt for userid and password. You can ctrl-c this Then install the created wheel","title":"Method 2:"},{"location":"#method-3","text":"No installation. Using the source code directly. If this is the case, you need to compile the protobufs. Run bash compile_protobufs.sh . Then add the project root to your $PYTHONPATH .","title":"Method 3:"},{"location":"#note","text":"Method 2 and 3 requires libprotoc>=3.6.0","title":"Note:"},{"location":"#why-model-server","text":"Taking deep learning models to production at scale is not a very straight forward process. If you are using Tensorflow then you have Tensorflow Serving . But, if you are using other frameworks like PyTorch , MXNet , scikit-learn etc. Taking your model to production is not very straight forward (Flask, Django and other ReST frameworks). Ideally you should be able to extend Tensorflow Serving to support models from other frameworks also but, this is extremly cumbersome! Thus, to bridge this gap we have Model-Server : A high performance framework neutral serving solution! The idea is: if you are able to train your model in python you should be able to deploy at scale using pure python","title":"Why Model-Server?"},{"location":"#salient-features","text":"Model-Server is heavily inspired from Tensorflow Serving Out of box client side batching support Pure python implementation : You don't need to fiddle around with C++ to have a scalable deployment solution Framework neutral : Using PyTorch , MXNet etc? Don't worry! The solution is platform neutral. If you can use a framework to train in python-3 , Model-Server can deploy it at scale Single server for multi-framework and multi-models : You can host multiple models using the same framework or a mixture of multiple PyTorch , MXNet , scikit-learn Tensorflow etc models!","title":"Salient Features"},{"location":"#getting-started","text":"The core of Model Server is a Servable . A servable is nothing but a python class containing your model's prediction definition which will be served by the Model-Server . All servables must inherit from model_server.Servable for the Model-Server to serve it. To deploy your model to production with Model Server , you just have to write a single python-3 file containing a class which inherits from model_server.Servable and has the following two methods: predict(self, input_array_dict) get_model_info(self, list_of_model_info_dict) Now run the floowing to start the server in 5001 port python -m model_server.runserver path_to_custom_servable_file.py For more info on command line arguments: python -m model_server.runserver --help","title":"Getting started"},{"location":"#a-simple-example","text":"create a file called simple_servable.py with the following contents: import numpy as np from model_server import Servable class my_custom_servable(Servable): def __init__(self, args): # args contains values from ArgumentParser # Thus you can pass any kwargs via command line and you get them here pass def predict(self, input_array_dict): \"\"\"This method is responsible for the gRPC call GetPredictions(). All custom servables must define this method. Arguments: input_array_dict (dict): The PredictionRequest proto decoded as a python dictionary. # example input_array_dict = { \"input_tensor_name1\": numpy array, \"input_tensor_name2\": numpy array } Returns: A python dictionary with key (typically output name) and value as numpy array of predictions # example output = { \"output_tensor_name1\": numpy array, \"output_tensor_name2\": numpy array } \"\"\" print(input_array_dict) return ({\"output_array1\": np.array([100, 200]).astype(np.float32), \"output_array2\": np.array([\"foo\".encode(),\"bar\".encode()]).astype(object), # you can get and pass strings encoded as bytes also }) def get_model_info(self, list_of_model_info_dict): \"\"\"This method which is responsible for the call GetModelInfo() Arguments: list_of_model_info_dict (list/tuple): A list containing model_info_dicts Note: model_info_dict contains the following keys: { \"name\": \"model name as string\" \"version\": \"version as string\" \"status\": \"status string\" \"misc\": \"string with miscellaneous info\" } Returns: list_of_model_info_dict (dict): containing the model and server info. This is similar to the function input \"\"\" return [{\"name\": \"first_model\", \"version\": 1, \"status\": \"up\"}, {\"name\": \"second_model\", \"version\": 2, \"status\": \"up\", \"misc\": \"Other miscellaneous details\"}] Now run: python -m model_server.runserver path/to/simple_servable.py To start the gRPC server! Now let's define the client! import grpc import numpy as np from model_server import server_pb2, server_pb2_grpc from model_server.utils import create_tensor_proto from model_server.utils import create_predict_request from model_server.utils import create_array_from_proto from model_server.utils import create_model_info_proto channel = grpc.insecure_channel('localhost:5001') # default port # create a stub (client) stub = server_pb2_grpc.ModelServerStub(channel) input_array_dict = {\"input1\":create_tensor_proto(np.array([1,2]).astype(np.uint8)), \"input2\":create_tensor_proto(np.array([[10.0,11.0], [12.0,13.0]]).astype(np.float32)), \"input3\":create_tensor_proto(np.array([\"Hi\".encode(), \"Hello\".encode(), \"test\".encode()]).astype(object)) } # create the prediction request predict_request= create_predict_request(input_array_dict, name=\"simple_call\") # make the call response = stub.GetPredictions(predict_request) # decode the response print(create_array_from_proto(response.outputs[\"output_array1\"])) # prints: array([100., 200.], dtype=float32) # Getting the model status model_info_proto = create_model_info_proto([]) # you can pass an empty list also response = stub.GetModelInfo(model_info_proto) Look at examples folder for further examples","title":"A simple example"},{"location":"#work-in-progress","text":"Support server side batching and async calls. Provide a gRPC endpoint for Active Learning so that you can plug in Model Server with your labeling tool and train on fly! Provide a ReST wrapper Feel free to file issues, provide suggestions and pull requests","title":"Work in Progress"},{"location":"model_server.core/","text":"Source: model_server/core.py#L0 Servable Helper class that provides a standard way to create an ABC using inheritance. Servable. __init__ __init__(self) Abstract base class for custom servables. All custom servables must inherit from this. All custom servables inheriting from this must implement the following methods: predict(self, input_array_dict) get_model_info(self, list_of_model_info_dict) Servable.get_model_info get_model_info(self, list_of_model_info_dict) Abstract method which is responsible for the call GetModelInfo Arguments: list_of_model_info_dict (list/tuple): A list containing model_info_dicts Note: model_info_dict contains the following keys: { \"name\": \"model name as string\" \"version\": \"version as string\" \"status\": \"status string\" \"misc\": \"string with miscellaneous info\" } Returns: list_of_model_info_dict (dict): containing the model and server info. This is similar to the function input Servable.predict predict(self, input_array_dict) Abstract method where the model prediction logic lives. This method is responsible for the gRPC call GetPredictions(). All custom servables must define this method. Arguments: input_array_dict (dict): The PredictionRequest proto decoded as a python dictionary. # example input_array_dict = { \"input_tensor_name1\": numpy array, \"input_tensor_name2\": numpy array } Returns: A python dictionary with key (typically output name) and value as numpy array of predictions # example output = { \"output_tensor_name1\": numpy array, \"output_tensor_name2\": numpy array } ModelServerServicer ModelServerServicer. __init__ __init__(self, custom_servable_object) gRPC Model Server Services. This is where the RPC methods are defined. Arguments: custom_servable_object : custom servable classe's instance ModelServerServicer.GetModelInfo GetModelInfo(self, request, context) Entrypoint for GetModelInfo gRPC call. Uses the get_model_info method defined in custom servable Arguments: request (protobuf): gRPC request containing input ModelInfo protobuf context (protobuf): gRPC context object Returns: ModelInfo protobuf ModelServerServicer.GetPredictions GetPredictions(self, request, context) Entrypoint for GetPredictions gRPC call. Uses the predict method defined in custom servable Arguments: request (protobuf): gRPC request containing input PredictRequest protobuf context (protobuf): gRPC context object Returns: PredictResponse protobuf","title":"Model server.core"},{"location":"model_server.core/#servable","text":"Helper class that provides a standard way to create an ABC using inheritance.","title":"Servable"},{"location":"model_server.core/#servable__init__","text":"__init__(self) Abstract base class for custom servables. All custom servables must inherit from this. All custom servables inheriting from this must implement the following methods: predict(self, input_array_dict) get_model_info(self, list_of_model_info_dict)","title":"Servable.__init__"},{"location":"model_server.core/#servableget_model_info","text":"get_model_info(self, list_of_model_info_dict) Abstract method which is responsible for the call GetModelInfo Arguments: list_of_model_info_dict (list/tuple): A list containing model_info_dicts Note: model_info_dict contains the following keys: { \"name\": \"model name as string\" \"version\": \"version as string\" \"status\": \"status string\" \"misc\": \"string with miscellaneous info\" } Returns: list_of_model_info_dict (dict): containing the model and server info. This is similar to the function input","title":"Servable.get_model_info"},{"location":"model_server.core/#servablepredict","text":"predict(self, input_array_dict) Abstract method where the model prediction logic lives. This method is responsible for the gRPC call GetPredictions(). All custom servables must define this method. Arguments: input_array_dict (dict): The PredictionRequest proto decoded as a python dictionary. # example input_array_dict = { \"input_tensor_name1\": numpy array, \"input_tensor_name2\": numpy array } Returns: A python dictionary with key (typically output name) and value as numpy array of predictions # example output = { \"output_tensor_name1\": numpy array, \"output_tensor_name2\": numpy array }","title":"Servable.predict"},{"location":"model_server.core/#modelserverservicer","text":"","title":"ModelServerServicer"},{"location":"model_server.core/#modelserverservicer__init__","text":"__init__(self, custom_servable_object) gRPC Model Server Services. This is where the RPC methods are defined. Arguments: custom_servable_object : custom servable classe's instance","title":"ModelServerServicer.__init__"},{"location":"model_server.core/#modelserverservicergetmodelinfo","text":"GetModelInfo(self, request, context) Entrypoint for GetModelInfo gRPC call. Uses the get_model_info method defined in custom servable Arguments: request (protobuf): gRPC request containing input ModelInfo protobuf context (protobuf): gRPC context object Returns: ModelInfo protobuf","title":"ModelServerServicer.GetModelInfo"},{"location":"model_server.core/#modelserverservicergetpredictions","text":"GetPredictions(self, request, context) Entrypoint for GetPredictions gRPC call. Uses the predict method defined in custom servable Arguments: request (protobuf): gRPC request containing input PredictRequest protobuf context (protobuf): gRPC context object Returns: PredictResponse protobuf","title":"ModelServerServicer.GetPredictions"},{"location":"model_server.utils.model_info_utils/","text":"Source: model_server/utils/model_info_utils.py#L0 create_model_info_proto create_model_info_proto(list_of_model_info_dict) Creates a ModelInfo proto Arguments: list_of_model_info_dict (list/tuple): A list containing model_info_dicts Note: model_info_dict contains the following keys: { \"name\": \"model name as string\" \"version\": \"version as string\" \"status\": \"status string\" \"misc\": \"string with miscellaneous info\" } Returns: ModelInfo proto decode_model_info_proto decode_model_info_proto(model_info_proto) Decodes the model_info_proto created by create_model_info_proto Arguments: model_info_proto (ModelInfo proto): model_info_proto created by create_model_info_proto Returns: list_of_model_info_dict (list): A list containing model_info_dicts Note: model_info_dict contains the following keys: { \"name\": \"model name as string\" \"version\": \"version as string\" \"status\": \"status string\" \"misc\": \"string with miscellaneous info\" }","title":"Model server.utils.model info utils"},{"location":"model_server.utils.model_info_utils/#create_model_info_proto","text":"create_model_info_proto(list_of_model_info_dict) Creates a ModelInfo proto Arguments: list_of_model_info_dict (list/tuple): A list containing model_info_dicts Note: model_info_dict contains the following keys: { \"name\": \"model name as string\" \"version\": \"version as string\" \"status\": \"status string\" \"misc\": \"string with miscellaneous info\" } Returns: ModelInfo proto","title":"create_model_info_proto"},{"location":"model_server.utils.model_info_utils/#decode_model_info_proto","text":"decode_model_info_proto(model_info_proto) Decodes the model_info_proto created by create_model_info_proto Arguments: model_info_proto (ModelInfo proto): model_info_proto created by create_model_info_proto Returns: list_of_model_info_dict (list): A list containing model_info_dicts Note: model_info_dict contains the following keys: { \"name\": \"model name as string\" \"version\": \"version as string\" \"status\": \"status string\" \"misc\": \"string with miscellaneous info\" }","title":"decode_model_info_proto"},{"location":"model_server.utils.prediction_utils/","text":"Source: model_server/utils/prediction_utils.py#L0 create_predict_request create_predict_request(input_tensorproto_dict, name=None, version=None) Creates a PredictRequest proto Arguments: input_tensorproto_dict (dict): A dictionary with key (typically input tensor name string) and value as TensorProto. name (str): Optional model name. Default None version (str): Optional model version. Default None Returns: PredictRequest proto create_predict_response create_predict_response(output_tensorproto_dict, name=None, version=None) Creates a PredictResponse proto Arguments: input_tensorproto_dict (dict): A dictionary with key (typically output tensor name string) and value as TensorProto. name (str): Optional model name. Default None version (str): Optional model version. Default None Returns: PredictResponse proto","title":"Model server.utils.prediction utils"},{"location":"model_server.utils.prediction_utils/#create_predict_request","text":"create_predict_request(input_tensorproto_dict, name=None, version=None) Creates a PredictRequest proto Arguments: input_tensorproto_dict (dict): A dictionary with key (typically input tensor name string) and value as TensorProto. name (str): Optional model name. Default None version (str): Optional model version. Default None Returns: PredictRequest proto","title":"create_predict_request"},{"location":"model_server.utils.prediction_utils/#create_predict_response","text":"create_predict_response(output_tensorproto_dict, name=None, version=None) Creates a PredictResponse proto Arguments: input_tensorproto_dict (dict): A dictionary with key (typically output tensor name string) and value as TensorProto. name (str): Optional model name. Default None version (str): Optional model version. Default None Returns: PredictResponse proto","title":"create_predict_response"},{"location":"model_server.utils.tensor_utils/","text":"Source: model_server/utils/tensor_utils.py#L0 create_tensor_proto create_tensor_proto(array, shape=None, dtype=None, name=None) Create a TensorProto from numpy array. Arguments: array (np.ndarray): The numpy array to convert to TensorProto shape (tuple): Optional shape of the array to reshape it to. If not given, it is inferred from the numpy array. Default None dtype (str): Optional dtype to convert the array to. Refer tensor.proto for supported datatypes. If not given, it is inferred from the numpy array. Default None. name (str): Optional name for the TensorProto. Default None Returns: A TensorProto containing the given array Note: python strings will be encoded to python bytes. Use dtype = \"object\" if the numpy array contains strings. dtype \"string\" and \"object\" are treated as same. \"string\" is converted to python \"object\". This is because, numpy handles variable length strings in this way create_array_from_proto create_array_from_proto(tensor_proto) Retrive array from TensorProto. Arguments: tensor_proto (TensorProto): An instance of TensorProto Returns: numpy array Note: python strings will be encoded to python bytes. Use dtype = \"object\" if the numpy array contains strings. dtype \"string\" and \"object\" are treated as same. \"string\" is converted to python \"object\". This is because, numpy handles variable length strings in this way","title":"Model server.utils.tensor utils"},{"location":"model_server.utils.tensor_utils/#create_tensor_proto","text":"create_tensor_proto(array, shape=None, dtype=None, name=None) Create a TensorProto from numpy array. Arguments: array (np.ndarray): The numpy array to convert to TensorProto shape (tuple): Optional shape of the array to reshape it to. If not given, it is inferred from the numpy array. Default None dtype (str): Optional dtype to convert the array to. Refer tensor.proto for supported datatypes. If not given, it is inferred from the numpy array. Default None. name (str): Optional name for the TensorProto. Default None Returns: A TensorProto containing the given array Note: python strings will be encoded to python bytes. Use dtype = \"object\" if the numpy array contains strings. dtype \"string\" and \"object\" are treated as same. \"string\" is converted to python \"object\". This is because, numpy handles variable length strings in this way","title":"create_tensor_proto"},{"location":"model_server.utils.tensor_utils/#create_array_from_proto","text":"create_array_from_proto(tensor_proto) Retrive array from TensorProto. Arguments: tensor_proto (TensorProto): An instance of TensorProto Returns: numpy array Note: python strings will be encoded to python bytes. Use dtype = \"object\" if the numpy array contains strings. dtype \"string\" and \"object\" are treated as same. \"string\" is converted to python \"object\". This is because, numpy handles variable length strings in this way","title":"create_array_from_proto"}]}