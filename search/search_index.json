{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Model-Server","text":"<p>A Pure <code>python-3</code> based flexible gRPC server for hosting Deep Learning, Machine Learning models trained on any framework! The documentation can be found here https://abhijit-2592.github.io/model-server/</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#directly-use-model-server-as-a-library","title":"Directly use model-server as a library","text":"<pre><code>pip install model-server\n</code></pre>"},{"location":"#developingimproving-model-server","title":"Developing/Improving Model Server","text":"<p>The project uses <code>uv</code> as the package manager</p> <pre><code>git clone https://abhijit-2592.github.io/model-server/\n\n# Sync in dev mode\nuv sync --dev\n\n# Install in editable mode\nuv pip install -e .\n</code></pre>"},{"location":"#why-model-server","title":"Why Model-Server?","text":"<p>Taking deep learning models to production at scale is not a very straight forward process. If you are using Tensorflow then you have Tensorflow Serving. But, if you are using other frameworks like PyTorch, MXNet, scikit-learn etc. Taking your model to production is not very straight forward (Flask, Django and other ReST frameworks). Ideally you should be able to extend Tensorflow Serving to support models from other frameworks also but, this is extremly cumbersome! Thus, to bridge this gap we have Model-Server: A high performance framework neutral serving solution! The idea is: if you are able to train your model in <code>python</code> you should be  able to deploy at scale using pure <code>python</code></p>"},{"location":"#salient-features","title":"Salient Features","text":"<p>Model-Server is heavily inspired from Tensorflow Serving</p> <ul> <li>Out of box client side batching support</li> <li>Pure python implementation: You don't need to fiddle around with C++ to have a scalable deployment solution</li> <li>Framework neutral:  Using PyTorch, MXNet etc? Don't worry! The solution is platform neutral. If you can use a framework to train in <code>python-3</code>, Model-Server can deploy it at scale</li> <li>Single server for multi-framework and multi-models: You can host multiple models using the same framework or a mixture of multiple PyTorch, MXNet, scikit-learn Tensorflow etc models!</li> </ul>"},{"location":"#getting-started","title":"Getting started","text":"<p>The core of Model Server is a <code>Servable</code>. A servable is nothing but a <code>python class</code> containing your model's prediction definition which will be served by the <code>Model-Server</code>. All servables must inherit from <code>model_server.Servable</code> for the  <code>Model-Server</code> to serve it.</p> <p>To deploy your model to production with <code>Model Server</code>, you just have to write a single <code>python-3</code> file containing a <code>class</code> which inherits from <code>model_server.Servable</code> and has the following two methods:</p> <pre><code>predict(self, input_array_dict)\nget_model_info(self, list_of_model_info_dict)\n</code></pre> <p>Now run the floowing to start the server in <code>5001</code> port <pre><code>python -m model_server.runserver path_to_custom_servable_file.py\n</code></pre></p> <p>For more info on  command line arguments: <pre><code>python -m model_server.runserver --help\n</code></pre></p>"},{"location":"#a-simple-example","title":"A simple example","text":"<p>create a file called <code>simple_servable.py</code> with the following contents: <pre><code>import numpy as np\nfrom model_server import Servable\n\n\nclass my_custom_servable(Servable):\n    def __init__(self, args):\n        # args contains values from ArgumentParser\n        # Thus you can pass any kwargs via command line and you get them here\n        pass\n\n    def predict(self, input_array_dict):\n        \"\"\"This method is responsible for the gRPC call GetPredictions().\n        All custom servables must define this method.\n\n        Arguments:\n            input_array_dict (dict): The PredictionRequest proto decoded as a python dictionary.\n\n        # example\n        input_array_dict = {\n                           \"input_tensor_name1\": numpy array,\n                           \"input_tensor_name2\": numpy array\n                            }\n\n        Returns:\n            A python dictionary with key (typically output name) and value as numpy array of predictions\n\n        # example\n        output = {\n                   \"output_tensor_name1\": numpy array,\n                   \"output_tensor_name2\": numpy array\n                  }\n        \"\"\"\n        print(input_array_dict)\n        return ({\"output_array1\": np.array([100, 200]).astype(np.float32),\n                 \"output_array2\": np.array([\"foo\".encode(),\"bar\".encode()]).astype(object),  # you can get and pass strings encoded as bytes also\n                 })\n\n    def get_model_info(self, list_of_model_info_dict):\n        \"\"\"This method which is responsible for the call GetModelInfo()\n\n        Arguments:\n            list_of_model_info_dict (list/tuple): A list containing model_info_dicts\n\n        Note:\n            model_info_dict contains the following keys:\n\n            {\n                \"name\": \"model name as string\"\n                \"version\": \"version as string\"\n                \"status\": \"status string\"\n                \"misc\": \"string with miscellaneous info\"\n            }\n\n        Returns:\n            list_of_model_info_dict (dict): containing the model and server info. This is similar to the function input\n        \"\"\"\n        return [{\"name\": \"first_model\", \"version\": 1, \"status\": \"up\"},\n                {\"name\": \"second_model\", \"version\": 2, \"status\": \"up\", \"misc\": \"Other miscellaneous details\"}]\n</code></pre></p> <p>Now run:</p> <p><pre><code>python -m model_server.runserver path/to/simple_servable.py\n</code></pre> To start the gRPC server!</p> <p>Now let's define the client!</p> <pre><code>import grpc\nimport numpy as np\n\nfrom model_server import server_pb2, server_pb2_grpc\nfrom model_server.utils import create_tensor_proto\nfrom model_server.utils import create_predict_request\nfrom model_server.utils import create_array_from_proto\nfrom model_server.utils import create_model_info_proto\n\nchannel = grpc.insecure_channel('localhost:5001')  # default port\n# create a stub (client)\nstub = server_pb2_grpc.ModelServerStub(channel)\ninput_array_dict = {\"input1\":create_tensor_proto(np.array([1,2]).astype(np.uint8)),\n                    \"input2\":create_tensor_proto(np.array([[10.0,11.0], [12.0,13.0]]).astype(np.float32)),\n                    \"input3\":create_tensor_proto(np.array([\"Hi\".encode(), \"Hello\".encode(), \"test\".encode()]).astype(object))\n                   }\n# create the prediction request\npredict_request= create_predict_request(input_array_dict, name=\"simple_call\")\n# make the call\nresponse = stub.GetPredictions(predict_request)\n\n# decode the response\nprint(create_array_from_proto(response.outputs[\"output_array1\"]))\n\n# prints: array([100., 200.], dtype=float32)\n\n# Getting the model status\n\nmodel_info_proto = create_model_info_proto([])  # you can pass an empty list also\nresponse = stub.GetModelInfo(model_info_proto)\n</code></pre> <p>Look at examples folder for further examples</p>"},{"location":"#work-in-progress","title":"Work in Progress","text":"<ul> <li>Support server side batching and async calls.</li> <li>Provide a gRPC endpoint for Active Learning so that you can plug in <code>Model Server</code> with your labeling tool and train on fly!</li> <li>Provide a ReST wrapper</li> </ul> <p>Feel free to file issues, provide suggestions and pull requests</p>"},{"location":"api_reference/core/","title":"model_server.core","text":""},{"location":"api_reference/core/#model_servercore","title":"model_server.core","text":""},{"location":"api_reference/core/#servable-objects","title":"Servable Objects","text":"<p> [view source] </p> <pre><code>class Servable(abc.ABC)\n</code></pre> <p></p>"},{"location":"api_reference/core/#__init__","title":"__init__","text":"<p> [view source] </p> <pre><code>def __init__()\n</code></pre> <p>Abstract base class for custom servables. All custom servables must inherit from this. All custom servables inheriting from this must implement the following methods:</p> <pre><code>predict(self, input_array_dict)\nget_model_info(self, list_of_model_info_dict)\n</code></pre> <p></p>"},{"location":"api_reference/core/#predict","title":"predict","text":"<p> [view source] </p> <pre><code>@abc.abstractmethod\ndef predict(input_array_dict)\n</code></pre> <p>Abstract method where the model prediction logic lives. This method is responsible for the gRPC call GetPredictions(). All custom servables must define this method.</p> <p>Arguments:</p> <ul> <li><code>input_array_dict</code> dict - The PredictionRequest proto decoded as a python dictionary.</li> </ul> <pre><code># example\ninput_array_dict = {\n                   \"input_tensor_name1\": numpy array,\n                   \"input_tensor_name2\": numpy array\n                    }\n</code></pre> <p>Returns:</p> <p>A python dictionary with key (typically output name) and value as numpy array of predictions</p> <pre><code># example\noutput = {\n           \"output_tensor_name1\": numpy array,\n           \"output_tensor_name2\": numpy array\n          }\n</code></pre> <p></p>"},{"location":"api_reference/core/#get_model_info","title":"get_model_info","text":"<p> [view source] </p> <pre><code>@abc.abstractmethod\ndef get_model_info(list_of_model_info_dict)\n</code></pre> <p>Abstract method which is responsible for the call GetModelInfo</p> <p>Arguments:</p> <ul> <li><code>list_of_model_info_dict</code> list/tuple - A list containing model_info_dicts</li> </ul> <p>Notes:</p> <p>model_info_dict contains the following keys:</p> <pre><code>```python\n{\n    \"name\": \"model name as string\"\n    \"version\": \"version as string\"\n    \"status\": \"status string\"\n    \"misc\": \"string with miscellaneous info\"\n}\n```\n</code></pre> <p>Returns:</p> <ul> <li><code>list_of_model_info_dict</code> dict - containing the model and server info. This is similar to the function input</li> </ul> <p></p>"},{"location":"api_reference/core/#modelserverservicer-objects","title":"ModelServerServicer Objects","text":"<p> [view source] </p> <pre><code>class ModelServerServicer(server_pb2_grpc.ModelServerServicer)\n</code></pre> <p></p>"},{"location":"api_reference/core/#__init___1","title":"__init__","text":"<p> [view source] </p> <pre><code>def __init__(custom_servable_object)\n</code></pre> <p>gRPC Model Server Services. This is where the RPC methods are defined.</p> <p>Arguments:</p> <ul> <li><code>custom_servable_object</code> - custom servable classe's instance</li> </ul> <p></p>"},{"location":"api_reference/core/#getpredictions","title":"GetPredictions","text":"<p> [view source] </p> <pre><code>def GetPredictions(request, context)\n</code></pre> <p>Entrypoint for GetPredictions gRPC call. Uses the predict method defined in custom servable</p> <p>Arguments:</p> <ul> <li><code>request</code> protobuf - gRPC request containing input PredictRequest protobuf</li> <li><code>context</code> protobuf - gRPC context object</li> </ul> <p>Returns:</p> <p>PredictResponse protobuf</p> <p></p>"},{"location":"api_reference/core/#getmodelinfo","title":"GetModelInfo","text":"<p> [view source] </p> <pre><code>def GetModelInfo(request, context)\n</code></pre> <p>Entrypoint for GetModelInfo gRPC call. Uses the get_model_info method defined in custom servable</p> <p>Arguments:</p> <ul> <li><code>request</code> protobuf - gRPC request containing input ModelInfo protobuf</li> <li><code>context</code> protobuf - gRPC context object</li> </ul> <p>Returns:</p> <p>ModelInfo protobuf</p>"},{"location":"api_reference/utils/","title":"model_server.utils","text":""},{"location":"api_reference/utils/#model_serverutilsprediction_utils","title":"model_server.utils.prediction_utils","text":""},{"location":"api_reference/utils/#create_predict_request","title":"create_predict_request","text":"<p> [view source] </p> <pre><code>def create_predict_request(input_tensorproto_dict, name=None, version=None)\n</code></pre> <p>Creates a PredictRequest proto</p> <p>Arguments:</p> <ul> <li><code>input_tensorproto_dict</code> dict - A dictionary with key (typically input tensor name string) and value as TensorProto.</li> <li><code>name</code> str - Optional model name. Default None</li> <li><code>version</code> str - Optional model version. Default None</li> </ul> <p>Returns:</p> <p>PredictRequest proto</p> <p></p>"},{"location":"api_reference/utils/#create_predict_response","title":"create_predict_response","text":"<p> [view source] </p> <pre><code>def create_predict_response(output_tensorproto_dict, name=None, version=None)\n</code></pre> <p>Creates a PredictResponse proto</p> <p>Arguments:</p> <ul> <li><code>input_tensorproto_dict</code> dict - A dictionary with key (typically output tensor name string) and value as TensorProto.</li> <li><code>name</code> str - Optional model name. Default None</li> <li><code>version</code> str - Optional model version. Default None</li> </ul> <p>Returns:</p> <p>PredictResponse proto</p> <p></p>"},{"location":"api_reference/utils/#model_serverutilstensor_utils","title":"model_server.utils.tensor_utils","text":""},{"location":"api_reference/utils/#create_tensor_proto","title":"create_tensor_proto","text":"<p> [view source] </p> <pre><code>def create_tensor_proto(array, shape=None, dtype=None, name=None)\n</code></pre> <p>Create a TensorProto from numpy array.</p> <p>Arguments:</p> <ul> <li><code>array</code> np.ndarray - The numpy array to convert to TensorProto</li> <li><code>shape</code> tuple - Optional shape of the array to reshape it to.   If not given, it is inferred from the numpy array. Default None</li> <li><code>dtype</code> str - Optional dtype to convert the array to.   Refer tensor.proto for supported datatypes.   If not given, it is inferred from the numpy array. Default None.</li> <li><code>name</code> str - Optional name for the TensorProto. Default None</li> </ul> <p>Returns:</p> <p>A TensorProto containing the given array</p> <p>Notes:</p> <ul> <li>python strings will be encoded to python bytes.</li> <li>Use dtype = \"object\" if the numpy array contains strings.</li> <li>dtype \"string\" and \"object\" are treated as same.</li> <li>\"string\" is converted to python \"object\". This is because,   numpy handles variable length strings in this way</li> </ul> <p></p>"},{"location":"api_reference/utils/#create_array_from_proto","title":"create_array_from_proto","text":"<p> [view source] </p> <pre><code>def create_array_from_proto(tensor_proto)\n</code></pre> <p>Retrive array from TensorProto.</p> <p>Arguments:</p> <ul> <li><code>tensor_proto</code> TensorProto - An instance of TensorProto</li> </ul> <p>Returns:</p> <p>numpy array</p> <p>Notes:</p> <ul> <li>python strings will be encoded to python bytes.</li> <li>Use dtype = \"object\" if the numpy array contains strings.</li> <li>dtype \"string\" and \"object\" are treated as same.</li> <li>\"string\" is converted to python \"object\". This is because,   numpy handles variable length strings in this way</li> </ul> <p></p>"},{"location":"api_reference/utils/#model_serverutilsmodel_info_utils","title":"model_server.utils.model_info_utils","text":""},{"location":"api_reference/utils/#create_model_info_proto","title":"create_model_info_proto","text":"<p> [view source] </p> <pre><code>def create_model_info_proto(list_of_model_info_dict)\n</code></pre> <p>Creates a ModelInfo proto</p> <p>Arguments:</p> <ul> <li><code>list_of_model_info_dict</code> list/tuple - A list containing model_info_dicts</li> </ul> <p>Notes:</p> <p>model_info_dict contains the following keys:</p> <pre><code>```python\n{\n    \"name\": \"model name as string\"\n    \"version\": \"version as string\"\n    \"status\": \"status string\"\n    \"misc\": \"string with miscellaneous info\"\n}\n```\n</code></pre> <p>Returns:</p> <p>ModelInfo proto</p> <p></p>"},{"location":"api_reference/utils/#decode_model_info_proto","title":"decode_model_info_proto","text":"<p> [view source] </p> <pre><code>def decode_model_info_proto(model_info_proto)\n</code></pre> <p>Decodes the model_info_proto created by create_model_info_proto</p> <p>Arguments:</p> <ul> <li><code>model_info_proto</code> ModelInfo proto - model_info_proto created by create_model_info_proto</li> </ul> <p>Returns:</p> <ul> <li><code>list_of_model_info_dict</code> list - A list containing model_info_dicts</li> </ul> <p>Notes:</p> <p>model_info_dict contains the following keys:</p> <pre><code>```python\n{\n    \"name\": \"model name as string\"\n    \"version\": \"version as string\"\n    \"status\": \"status string\"\n    \"misc\": \"string with miscellaneous info\"\n}\n```\n</code></pre>"}]}